\chapter{Introduction}
Arguably the most famous open problem in computer science is the question of whether or not \textit{\textbf{P=NP}}. At a high level, it is the question of whether all problems for which an algorithm can \textit{quickly} check whether a specific answer is correct must also permit a fast algorithm that finds a correct answer from scratch. If anyone is able to solve the problem, they would be given a \$1 million cash prize from the Clay Mathematics Institute, as well as be inducted into Computer Science history. While we will not attempt to solve this infamous problem, we will continue to build on the shoulders of giants and add to the existing understanding of these complexity classes. In particular, we will focus on $k$-best optimization problems and expand the collective knowledge of these problems by specifically focusing on existing \nph{} problems. Before we can do that, though, we must define some terminology.
\section{Preliminaries}
\subsection{P and NP}
Let us first define our problem space. Formally, \textit{\textbf{\pp{}}} is the set of problems that a deterministic Turing Machine can solve in an amount of processes that is polynomial in the size of the input. The word ``deterministic" is key in this context, as it means the machine must only be able to perform a single action at once, decided entirely by its state and programming. On the other hand, if a problem is solvable by a non-deterministic Turing Machine in polynomial time on the size of the input, then it is considered to be in \textit{\textbf{\np{}}}. A non-deterministic Turing Machine is capable of performing multiple actions at once. In modern computing, it can be thought of as a perfectly parallel processor, but the important thing to note is that a non-deterministic Turing Machine, when choosing between possible paths down its state tree, will always choose the path that eventually leads to the optimal solution -- at least when one exists. While this non-deterministic machine is the original idea that creates the class of $\np{}$ problems, it is often easier to consider the equivalent formulation of a verifier. A problem is $\np{}$ if any algorithm exists that can, in polynomial time on the size of the input, confirm that a provided feasible solution is an actual solution to a problem. Thus, intuitively, non-deterministic machines seem explicitly more capable than polynomial machines. They can ``guess" the answer to a question, and be sure that they are correct. This leads many researchers to the hypothesis that $\pp{} \subsetneq \np{}$, or that every $\pp{}$ problem must be solvable by a non-deterministic machine but that the reverse is not true. However, this belief has yet to be rigorously proven, and many computer scientists are still actively searching for polynomial time algorithms that break the mold and could prove that, in fact, $\pp{} = \np{}$. 
\subsection{NP-Hardness}
In the world of $\np{}$, there are two especially interesting classes of problems. Since non-deterministic polynomial machines are certainly not in any way weaker than their deterministic counterparts, it is obvious that any problem in $\pp{}$ must also be in $\np{}$, i.e. $\pp \subseteq \np$. This leads to the natural distinction of problems that are in $\np{}$ and yet likely not in \pp. One way to define problems that meet this description is with the \textit{\textbf{\nph{}}} problems. This is a set of computable problems that are ``at least as hard" as all other problems in $\np{}$. That means that if we find a way to solve any of these problems in polynomial time with a deterministic machine, we can solve every problem in $\np{}$ in polynomial time, and thus $\pp{} = \np{}$. This class of $\nph{}$ problems is constantly growing as more and more researchers find new tasks that fall under the category. The proof that a problem is $\nph{}$ is not necessarily complicated. If we want to prove that problem $B$ is $\nph{}$, we would need to prove that there is a polynomial time deterministic algorithm that can convert the input of an existing $\nph{}$ problem, $A$, into that of the examined problem $B$. If we can then show that answer of the produced input on $B$ is the same as answer to the original input for problem $A$, we have shown that $B$ is an $\nph{}$ problem. This ``conversion" algorithm is referred to as a many-one reduction, or a Karp reduction. An alternative reduction, called a Turing reduction, is also occasionally used in complexity analysis. Unlike Karp reductions, Turing reductions permit the repeated use of an algorithm that solves $B$ in our attempt to solve $A$. In other words, Turing reductions can be thought of as programs that can use a subroutine -- often called an oracle -- that solves our examined problem $B$ in order to solve an existing $\nph{}$ problem $A$ in polynomial time. This makes Turing reductions a superset of Karp reductions; every Karp reduction is a Turing reduction but not necessarily the other way around. This does however make Turing reductions more powerful which in turn makes them less distinguishing, and thus Karp reductions have become the gold standard in complexity analysis. Where possible, and unless otherwise noted, this thesis will use Karp reductions. 
\subsection{NP-Completeness}
In the above section, we mentioned that $\nph{}$ problems are at least as hard as all problems in $\np{}$. As this statement implies, there are problems that are even more difficult than \np{}. Even non-deterministic Turing Machines can't solve these types of problems efficiently. While this thesis will not deal extensively with these problems, they do necessitate a final distinction, the \textit{\textbf{\npc{}}} class of problems. These are simply problems that are both $\nph{}$ and $\np{}$, that is they can be solved in polynomial time by a non-deterministic machine, but they also can be used to quickly solve all other problems in \np{}, thus likely seperating them from all of the problems in $\pp{}$, so long as $\pp{} \neq \np{}$.
\subsection{Decision and Optimization Problems}
We have so far defined the concept of a computational ``problem" somewhat loosely, so let us deal with this term more rigorously. We will deal with two particular types of problems. The first are \textbf{decision problems}. Given a general language $L$ that contains all input strings valid for our problem, the language of a decision problem takes the following form: $\{{} I \in L : \mathcal{P}(I)\}$ where $\mathcal{P}$ is a predicate that is either true or false on all input strings in $L$. In other words, it is the set of all strings in $L$ that are true under the predicate of the problem. Intuitively, decision problems can be thought of as problems that are solved by an algorithm that accepts inputs of a certain type and returns exclusively true or false. All $\npc{}$ problems are of this form. We can greatly expand the number of interesting problems if we consider \textbf{optimization problems}, as these are some of the most common to come about in the real world. Consider instead a problem in which we have a set of possible solutions $S$, and we are given a cost function $C : S \rightarrow{} \mathbb{R}$. An optimization problem is a problem in which the machine must determine the solution $s \in S$ such that $C(s)$ is either minimized or maximized. Any maximization or minimization optimization problem can be easily reduced into the other with a simple sign flip of the cost function, so we will henceforth simply refer to the minimization of the cost function when considering general optimization problems. Specific problem instances for which maximization is more natural will remain unchanged, so this note is only important when discussing the problems in a general sense. While optimization problems cannot themselves be considered $\npc{}$, we can often craft a decision problem from these. The most common way of doing this is to use the same formulation as the optimization problem, but add an parameter $t$ into the input, so that instead of asking the algorithm to optimize, we ask whether or not there is a solution with cost lower than $t$. Unless otherwise stated, any time we refer to the ``decision form" of what is otherwise an optimization problem, we are referring to this formulation. In the world of optimization problems, we can even further specify \textbf{combinitorial optimization problems}. These are optimization problems in which the set of feasible solutions, $S$, is discrete. Combinitorial optimization problems will be the primary focus for this thesis, as it is only for these types of problems that there is a natural definition of $k$-best solutions.
\subsection{$k$-Best Combinitorial Optimization Problems}
Now that we have most of the necessary terminology available, we can look more closely at the subject of this thesis. While for some problems, we really only do care about the one-and-only optimal solution, for many real world situations this is not the case. This is where $k$-best algorithms come into play. Instead of taking in the standard input for an optimization problem and searching for the optimal solution, a $k$-best algorithm will also have an enumeration value, $k$, and will output the top $k$ optimal solutions according to the cost function. At first, it may seem useless to do so much additional work just to find worse solutions, but there are plenty of instances where it is helpful. Recently in my own life, I was planning a trip through Europe with some of my friends for after graduation. It involved possible routes between dozens of different cities. I didn't want to just find the single cheapest or shortest route between all these different locations, I also wanted to compare that ``best" route with other good routes to see how much better it was, and whether there were other factors I could consider that might make the ``good, but suboptimal" solutions even better than the ``optimal" solution. This is just one application of a $k$-best combinitorial optimization problem, and is functionally equivalent to the $k$-best travelling salesman problem discussed later in this thesis.

Beyond personal experience, in many real-world cases the optimal solution can be unstable or fragile; small changes to the initial conditions could drastically alter the optimality of a solution. Having a collection of very good options instead of one single solution could improve the robustness of the algorithm, preventing small changes from creating large-scale issues \cite{kurtz2023approximation, kalyanmoy2006in}. Some areas of study that this may be particularly useful are in machine learning, internet network routing, and transportation routing, where it is not uncommon for small changes in the system can lead to sweeping changes in the outcome if not properly accounted for. 

In the end, most computer models can be considered an approximation of what is happening in the real world, thus making it important to compare the calculated optimal solution against its rivals. 
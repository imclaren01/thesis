\section{Related Work}
The study of second-best and $k$-best solutions is closely tied to the overarching field of computational complexity theory. The seminal works of Cook \cite{cook1971complexity}, Levin \cite{levin1973universal}, and Karp \cite{karp1972reducibility} established the foundations of reducibility, NP-hardness, and NP-completeness. In order to understand the hardness of $k$-best problems, we must first have a detailed understanding of the basis of the field and the various existing complexity classes. Garey and Johnson's book \cite{garey1979computers} serves as a comprehensive guide to the theory of NP-completeness and many of the known problems that have no efficient solutions, assuming that indeed $P \neq NP$. Finally, the famous, or perhaps infamous, ``Introduction to Algorithms" by Cormen et al. \cite{CLRS} textbook is a must-have for any researcher in algorithms and complexity theory.

Delving specifically into our focus, the problems of finding the $2nd$-best or $k$-best solutions to combinatorial optimization problems date back decades. Four years before Richard Karp produced the previously mentioned paper defining the class of NP-Complete problems, Murty published an establishing work in the field of $k$-best problems, showing that the assignment problem, a well-established problem in P, permits an efficient algorithm to enumerate the $k$-best solutions \cite{murty1968letter}. Three years later, in 1971, Yen is able to do the same for the shortest path problem, another problem with a known polynomial algorithm for finding the optimal solution. This algorithm is still highly useful to this day, as there are many applications in which it may be helpful to see a handful of the shortest paths through a graph, for instance when selecting a route on Google Maps that differs from the fastest route. Importantly, the time complexity of each of these algorithms is at least linear in $k$. Since $k$ is an integer, and binary encodings of integers grow exponentially in the size of the integer, the time complexity of these algorithms is technically also exponential in input size. This is often referred to as a pseudopolynomial time algorithm. It may be an interesting area of further study to constrain $k$ not just to the $2nd$-best, but to some sort of theoretical ``breaking point", where a problem no longer has a strongly polynomial time algorithm to find the $k$th-best solution.


One year after Yen's paper, in 1972, Lawler published one of the most important papers ever produced in the field \cite{lawler1972procedure}. In his work, he is able to expand the previous two approaches into a general approach for handling $k$-best enumeration problems. He describes a procedure to enumerate solutions in cost order by efficiently solving subproblems at each step.

Since then, numerous researchers have built upon these initial findings and developed more efficient algorithms for specific combinatorial optimization problems, as well as for problems in which Lawler's procedure isn't easily applicable. Gabow \cite{gabow1977finding} presented two algorithms for generating weighted spanning trees in order, which is equivalent to generating the $k$-best minimum spanning trees. His work introduced the practical concept of using a priority queue to efficiently generate solutions in increasing order of cost, a technique that has been widely adopted in later research, as well as expanded upon and improved for common constrained types of graphs \cite{katoh1981efficient, frederickson1993ambivalent}. Eppstein, a name which will come up all throughout this thesis for his constant efforts to expand recognition for $k$-best algorithms, \cite{eppstein1997finding} developed an algorithm for finding the $k$ shortest paths in a graph, improving upon Yen's earlier work in the area. His algorithm, which runs in $O(m + n + k)$ time when provided a shortest path tree and $O(m + nlog(n) + klog(k))$ otherwise, where $m$ is the number of edges and $n$ is the number of vertices in the graph, is particularly useful to assist with efficiently solving a variety of important dynamic programming problems.

The complexity of finding the second-best solution has also been studied for the minimum spanning tree problem. Katoh et al. \cite{doi:10.1137/0210017} presented an algorithm for finding the $k$ minimum spanning trees in an undirected graph. Their algorithm runs in $O(km + \min(n^2, m\log \log n))$ time and uses $O(k + m)$ space, where $n$ is the number of vertices and $m$ is the number of edges. This result shows that finding the second-best minimum spanning tree can be done efficiently for a fixed value of $k$.

However, the complexity changes when $k$ is part of the input. Ravi et al. \cite{ravi1996spanning} proved that finding the $k$th best minimum spanning tree is NP-hard for arbitrary values of $k$. This hardness result holds even for planar graphs and implies that, unless P=NP, there is no polynomial-time algorithm for finding the $k$-best minimum spanning trees when $k$ is not fixed.

These results highlight the interesting complexity landscape of finding second-best solutions, where the difficulty can depend on whether certain parameters, such as the value of $k$, are fixed or part of the input. The contrast between the polynomial-time algorithm for fixed $k$ and the NP-hardness for arbitrary $k$ in the case of minimum spanning trees provides a compelling example of this phenomenon.

The complexity of finding the second-best solution has also been studied for lesser-known problems. Camerini \cite{camerini1980complexity} generalized the existing work on weighted spanning tree enumeration into a paper on $k$-best spanning arborsecenses. Fukuda et al. was able to improve upon existing techniques to develop a much more efficient method to enumerate all perfect matchings in a bipartite graph \cite{fukuda1997finding}.

In addition to problem-specific algorithms, researchers have also developed general techniques for $k$-best enumeration. Fourty-eight years after Lawler publishes his generalized procedure to approach $k$-best problems, Eppstein \cite{eppstein2014k} provided an overview of $k$-best enumeration techniques and their applications, highlighting the importance of these methods in various practical fields especially in social, road, and internet networks. This survey was key in developing our own thoughts on the field and helped lead to the creation of this thesis. It is an excellent starting place for a new researcher in this field, and even deep into research I found myself returning to it as a useful reference. At the same time, it also shows how little research has previously gone into examining $k$-best $\nph{}$ problems, both in terms of developing better algorithms and in terms of establishing the complexity classes of such problems. The only reference in the survey to research into $k$-best problems where the original optimization problem is $\nph{}$ is referencing a technique commonly used in $\nph{}$ problems where we deal with ``parametrized complexity", where we can find a polynomial algorithm in some integer that defines the size of the input. Chen et al. use this technique to enumerate the top $k$ solutions to various $\nph{}$ problems \cite{chen2013par}, however their solutions require heavily constrained inputs and don't deal much in the actual hardness of the problem beyond the initial optimization problem.

Moving beyond Eppstein's work, Hamacher and Queyranne \cite{hamacher1982k} analyzed the existing Lawler approach and established their own procedure to enumerate the $k$-best solutions of various problems, which has been widely cited and extended in subsequent research \cite{yen2005finding, pascoal2013k}.

Other notable contributions to the field include the work of Papadimitriou and Steiglitz \cite{papadimitriou1982complexity} on the complexity of combinatorial optimization, which provides a detailed analysis of the algorithmic and complexity-theoretic aspects of optimization problems. Papadimitriou's work will be referenced multiple times in this thesis, particularly in the section on the Travelling Salesman Problem. There were several moments during the research process that we found a result only to read the following week that Papadimitriou has already proven something that was nearly the same more than 30 years prior. Valiant's work \cite{valiant1979complexity} on the complexity of enumeration and reliability problems has laid the foundation for the study of counting problems and their relationship to optimization.

In addition to the works mentioned above, there are many other research efforts that have been directed toward finding near-optimal solutions in combinatorial optimization, a similar but distinct problem. Powerful and complex techniques, both general and problem-specific, such as local search, approximation algorithms, and integer programming formulations, show the practicality of being able to rapidly find ``good" solutions that are not necessary optimal.

From this thought, some fields have been created and taken down entirely new avenues of research. One such field is matheuristics, a relatively niche study that I have only recently come across, in which a heuristic algorithm is combined with a solution improvement procedure using machine learning \cite{de2023mathematical}. A more well-known field, at least when it comes to the study of complexity theory, is the search for approximation algorithms and the study of NP-hard optimization using approximation algorithms. Particularly in this section, there have been a multitude of results pertaining to many of the problems covered later in this thesis, these include Max-Clique \cite{uriel2004approx}, Vertex-Cover \cite{karakostas2009better}, Independent Set \cite{chalermsook2023polynomial, berman1994approximating}, Set-Cover \cite{shi2019approx}, and Bin-Packing \cite{zhang2000linear}. Initially, we suspected there would be a correlation, if not a causation, between the difficulty of approximation for a problem and the difficulty of finding the second-best solution. While we weren't able to make conclusive headway in that area, we do still suspect some connection and will leave it for future work in the field to prove it one way or the other. In general, the sheer amount of work and results in related fields both illustrates the value of studying $k$-best problems and highlights the complexity and difficulty of working in the field. We had many questions to start the research, and were only able to answer some of them, while many others remain to be delved into by further research.  

\section{Established Approaches}
There are several approaches demonstrated by previous research in the field. Eppstein does an excellent job outlining them in \cite{eppstein2014k}, and we will summarize them here. 
\subsection{Optimal Substructures}
The optimal substructure approach leverages the principles of dynamic programming to solve $k$-best problems efficiently. In this method, the original problem is decomposed into a collection of subproblems, each of which is a smaller instance of the first problem. The key insight that permits the use of dynamic programming is that the optimal solutions to the original problem can be constructed by combining the optimal solutions of these subproblems in a specific manner, as described by a recurrence relation. We can use this fact to develop a similar technique to enumerate more than one good solution.

To apply this technique to $k$-best problems, instead of maintaining a single optimal solution for each subproblem, we store each of the $k$ best solutions. The recurrence relation is then adapted to express the $k$ best solutions of a subproblem in terms of the $k$ best solutions of smaller subproblems. Thus, we can use the dynamic programming algorithm to solve our own problem. At each step moving from smaller to larger subproblems, the recurrence relation is used to compute and store the $k$ best solutions for the current subproblem, based on the previously computed solutions of smaller subproblems.

This approach effectively extends the dynamic programming paradigm to allow the generation of multiple optimal solutions, enabling the efficient computation of $k$-best results for problems exhibiting optimal substructure properties.

\subsection{Solution-Space Partitions}
The solution-space partitioning approach tackles $k$-best problems by recursively dividing the solution space into smaller regions, each corresponding to a more constrained subproblem. This is distinct from the previous technique, which uses the original input to create smaller instances of the same problem. Here, we explicitly constrain the possible solutions, essentially redefining the problem in some manner. This method exploits the fact that the best and second-best solutions to a problem must differ in some respect, such as the inclusion or exclusion of a particular element. In this manner, we are able to effectively design a solution tree in which the problems become more and more restricted as we move farther down the tree.

There are two ways to design such an algorithm. One is used when it is relatively easy to find the second-best solution to any subproblem, and one when doing so is comparatively difficult. In the first situation, we can move beyond creating a solution tree into creating a solution heap, or a binary tree, where every child node (subproblem) is worse than its parent node (subproblem) and every parent has at most two children. We create this tree using the fact that the best and second-best solutions must have some distinguishing characteristic. For instance, in a graph problem, they might differ by the inclusion or exclusion of a particular edge or set of edges. By identifying this distinguishing element, we can create two child subproblems: one where the distinguishing element is mandated to be part of the solution, and another where it is prohibited. Consequently, one child subproblem will contain the best solution and the other must not contain the best solution. Since the parent node is unrestricted, it will always be at least as good as both of its children, and its children must be distinct from each other.

The binary tree structure then enables the application of efficient selection algorithms, such as Frederickson's \cite{frederickson1993optimal}, to identify the $k$ best solutions by exploring only $O(k)$ subproblems along the binary tree. As a result, the overall time complexity of this approach is $O(k)$ multiplied by the time required to find the second-best solution for a given subproblem.

If we aren't able to efficiently calculate the second-best solution, things get more complicated. An original example of this approach was defined by Lawler in his aforementioned work \cite{lawler1972procedure}. It is not dissimilar to the other approach, but in this case we aren't able to store the second-best solutions and thus we aren't able to generate a binary tree. Instead, we create a heap-ordered tree where we cannot guarantee that there are only two children for each node. In this approach, the best solution is represented as a sequence of elements, and the solution space is partitioned into subproblems based on prefixes of this sequence. Each subproblem includes a prefix of the best solution's sequence and excludes the next element. The resulting subproblems form a heap-ordered tree, with the branching factor equal to the maximum number of elements in a solution. A selection algorithm is then used to identify the $k$ best solutions. Due to the much higher branching factor, this method must examine $ O(ks) $ subproblems, where $s$ is the maximum size of a solution, yielding a total time complexity of $O(ks)$ times the time required to find the best solution.

\section{Contribution}
This thesis aims to substantially increase the studied realm of $k$-best enumeration problems. Most of the existing literature focuses on polynomial time optimization problems, and even there past literature has shown interesting results. For instance, as mentioned above, it has been proven that for any specific polynomial $k$, the $k$th-best version of the minimum spanning tree problem is solvable in polynomial $k$, however for a general $k$, it becomes provably $\nph{}$. These results led us to the conclusion that there must be new areas to explore existing under the hood of $k$-best problems. Since $k$-best problems are fairly well studied when it comes to known polynomial-time solvable problems, we decided to focus more heavily on $\nph{}$ problems to expand into new territory. In particular, we largely limit our focus to the specific problem of finding the second-best solution to an optimization problem. This allows us to avoid complications with the exponential growth of $k$ when used as an input, as well as allowing for an easier jumping-off point for future research into $k$-best $\nph{}$ problems. We will introduce several versions of the second best problem category, as well as proving the \nph{ness} of many selected second-best problems. We will also provide several general approaches for proving the hardness of similar second-best combinitorial optimization problems outside the scope of what we have shown here. We also study the various second-best forms of the traveling salesman problem in particular in more depth, as it has some interesting features and use cases for second and $k$-best algorithms. Finally, we will attempt to create an intuition for what makes a second-best problem $\nph{}$ and show instances of problems where the relative difficulty to find the next best option is clear.

Our work builds upon these previous studies by investigating the complexity of finding second-best solutions for a wide range of NP-hard combinatorial optimization problems. We extend the existing knowledge by providing a unified framework for analyzing the hardness of second-best problems and exploring the connections between the approximability of a problem and the difficulty of finding its second-best solution. By studying the complexity of second-best solutions, we aim to shed light on the structure of optimization problems and contribute to the development of more efficient algorithms for finding near-optimal solutions. Our thesis also serves to not only initiate the formal study of this interesting class of problems and formalize the problem space, but additionally highlights the value and real-world importance of studying this area and how it relates to many other research directions. Through this, we aim to not only present our new results but also to open up new directions for research that we hope will prove fruitful.